{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Web Scrapping of Reddit post through PRAW API*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping r/depression...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/depression\n",
      "  Scraped 200 posts from r/depression\n",
      "  Scraped 300 posts from r/depression\n",
      "  Scraped 400 posts from r/depression\n",
      "  Scraped 500 posts from r/depression\n",
      "  Scraped 600 posts from r/depression\n",
      "  Scraped 700 posts from r/depression\n",
      "  Scraped 800 posts from r/depression\n",
      "  Scraped 900 posts from r/depression\n",
      "  Using method: hot (time_filter: None)\n",
      "  Scraped 1000 posts from r/depression\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1100 posts from r/depression\n",
      "  Scraped 1200 posts from r/depression\n",
      "  Scraped 1300 posts from r/depression\n",
      "  Scraped 1400 posts from r/depression\n",
      "  Scraped 1500 posts from r/depression\n",
      "  Scraped 1600 posts from r/depression\n",
      "  Scraped 1700 posts from r/depression\n",
      "  Scraped 1800 posts from r/depression\n",
      "  Scraped 1900 posts from r/depression\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/depression\n",
      "  Scraped 2100 posts from r/depression\n",
      "  Scraped 2200 posts from r/depression\n",
      "  Scraped 2300 posts from r/depression\n",
      "  Scraped 2400 posts from r/depression\n",
      "  Scraped 2500 posts from r/depression\n",
      "  Scraped 2600 posts from r/depression\n",
      "  Scraped 2700 posts from r/depression\n",
      "  Scraped 2800 posts from r/depression\n",
      "  Scraped 2900 posts from r/depression\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/depression\n",
      "  Scraped 3100 posts from r/depression\n",
      "  Scraped 3200 posts from r/depression\n",
      "  Scraped 3300 posts from r/depression\n",
      "  Scraped 3400 posts from r/depression\n",
      "  Scraped 3500 posts from r/depression\n",
      "  Scraped 3600 posts from r/depression\n",
      "  Scraped 3700 posts from r/depression\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/depression. Total posts: 3783\n",
      "Scraping r/anxiety...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/anxiety\n",
      "  Scraped 200 posts from r/anxiety\n",
      "  Scraped 300 posts from r/anxiety\n",
      "  Scraped 400 posts from r/anxiety\n",
      "  Scraped 500 posts from r/anxiety\n",
      "  Scraped 600 posts from r/anxiety\n",
      "  Scraped 700 posts from r/anxiety\n",
      "  Scraped 800 posts from r/anxiety\n",
      "  Scraped 900 posts from r/anxiety\n",
      "  Using method: hot (time_filter: None)\n",
      "  Scraped 1000 posts from r/anxiety\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1100 posts from r/anxiety\n",
      "  Scraped 1200 posts from r/anxiety\n",
      "  Scraped 1300 posts from r/anxiety\n",
      "  Scraped 1400 posts from r/anxiety\n",
      "  Scraped 1500 posts from r/anxiety\n",
      "  Scraped 1600 posts from r/anxiety\n",
      "  Scraped 1700 posts from r/anxiety\n",
      "  Scraped 1800 posts from r/anxiety\n",
      "  Scraped 1900 posts from r/anxiety\n",
      "  Scraped 2000 posts from r/anxiety\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2100 posts from r/anxiety\n",
      "  Scraped 2200 posts from r/anxiety\n",
      "  Scraped 2300 posts from r/anxiety\n",
      "  Scraped 2400 posts from r/anxiety\n",
      "  Scraped 2500 posts from r/anxiety\n",
      "  Scraped 2600 posts from r/anxiety\n",
      "  Scraped 2700 posts from r/anxiety\n",
      "  Scraped 2800 posts from r/anxiety\n",
      "  Scraped 2900 posts from r/anxiety\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/anxiety\n",
      "  Scraped 3100 posts from r/anxiety\n",
      "  Scraped 3200 posts from r/anxiety\n",
      "  Scraped 3300 posts from r/anxiety\n",
      "  Scraped 3400 posts from r/anxiety\n",
      "  Scraped 3500 posts from r/anxiety\n",
      "  Scraped 3600 posts from r/anxiety\n",
      "  Scraped 3700 posts from r/anxiety\n",
      "  Scraped 3800 posts from r/anxiety\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/anxiety. Total posts: 3824\n",
      "Scraping r/mentalhealth...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/mentalhealth\n",
      "  Scraped 200 posts from r/mentalhealth\n",
      "  Scraped 300 posts from r/mentalhealth\n",
      "  Scraped 400 posts from r/mentalhealth\n",
      "  Scraped 500 posts from r/mentalhealth\n",
      "  Scraped 600 posts from r/mentalhealth\n",
      "  Scraped 700 posts from r/mentalhealth\n",
      "  Scraped 800 posts from r/mentalhealth\n",
      "  Scraped 900 posts from r/mentalhealth\n",
      "  Using method: hot (time_filter: None)\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1000 posts from r/mentalhealth\n",
      "  Scraped 1100 posts from r/mentalhealth\n",
      "  Scraped 1200 posts from r/mentalhealth\n",
      "  Scraped 1300 posts from r/mentalhealth\n",
      "  Scraped 1400 posts from r/mentalhealth\n",
      "  Scraped 1500 posts from r/mentalhealth\n",
      "  Scraped 1600 posts from r/mentalhealth\n",
      "  Scraped 1700 posts from r/mentalhealth\n",
      "  Scraped 1800 posts from r/mentalhealth\n",
      "  Scraped 1900 posts from r/mentalhealth\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/mentalhealth\n",
      "  Scraped 2100 posts from r/mentalhealth\n",
      "  Scraped 2200 posts from r/mentalhealth\n",
      "  Scraped 2300 posts from r/mentalhealth\n",
      "  Scraped 2400 posts from r/mentalhealth\n",
      "  Scraped 2500 posts from r/mentalhealth\n",
      "  Scraped 2600 posts from r/mentalhealth\n",
      "  Scraped 2700 posts from r/mentalhealth\n",
      "  Scraped 2800 posts from r/mentalhealth\n",
      "  Scraped 2900 posts from r/mentalhealth\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/mentalhealth\n",
      "  Scraped 3100 posts from r/mentalhealth\n",
      "  Scraped 3200 posts from r/mentalhealth\n",
      "  Scraped 3300 posts from r/mentalhealth\n",
      "  Scraped 3400 posts from r/mentalhealth\n",
      "  Scraped 3500 posts from r/mentalhealth\n",
      "  Scraped 3600 posts from r/mentalhealth\n",
      "  Scraped 3700 posts from r/mentalhealth\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/mentalhealth. Total posts: 3728\n",
      "Scraping r/BPD...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/BPD\n",
      "  Scraped 200 posts from r/BPD\n",
      "  Scraped 300 posts from r/BPD\n",
      "  Scraped 400 posts from r/BPD\n",
      "  Scraped 500 posts from r/BPD\n",
      "  Scraped 600 posts from r/BPD\n",
      "  Scraped 700 posts from r/BPD\n",
      "  Scraped 800 posts from r/BPD\n",
      "  Scraped 900 posts from r/BPD\n",
      "  Using method: hot (time_filter: None)\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1000 posts from r/BPD\n",
      "  Scraped 1100 posts from r/BPD\n",
      "  Scraped 1200 posts from r/BPD\n",
      "  Scraped 1300 posts from r/BPD\n",
      "  Scraped 1400 posts from r/BPD\n",
      "  Scraped 1500 posts from r/BPD\n",
      "  Scraped 1600 posts from r/BPD\n",
      "  Scraped 1700 posts from r/BPD\n",
      "  Scraped 1800 posts from r/BPD\n",
      "  Scraped 1900 posts from r/BPD\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/BPD\n",
      "  Scraped 2100 posts from r/BPD\n",
      "  Scraped 2200 posts from r/BPD\n",
      "  Scraped 2300 posts from r/BPD\n",
      "  Scraped 2400 posts from r/BPD\n",
      "  Scraped 2500 posts from r/BPD\n",
      "  Scraped 2600 posts from r/BPD\n",
      "  Scraped 2700 posts from r/BPD\n",
      "  Scraped 2800 posts from r/BPD\n",
      "  Scraped 2900 posts from r/BPD\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/BPD\n",
      "  Scraped 3100 posts from r/BPD\n",
      "  Scraped 3200 posts from r/BPD\n",
      "  Scraped 3300 posts from r/BPD\n",
      "  Scraped 3400 posts from r/BPD\n",
      "  Scraped 3500 posts from r/BPD\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/BPD. Total posts: 3579\n",
      "Scraping r/SuicideWatch...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/SuicideWatch\n",
      "  Scraped 200 posts from r/SuicideWatch\n",
      "  Scraped 300 posts from r/SuicideWatch\n",
      "  Scraped 400 posts from r/SuicideWatch\n",
      "  Scraped 500 posts from r/SuicideWatch\n",
      "  Scraped 600 posts from r/SuicideWatch\n",
      "  Scraped 700 posts from r/SuicideWatch\n",
      "  Scraped 800 posts from r/SuicideWatch\n",
      "  Scraped 900 posts from r/SuicideWatch\n",
      "  Using method: hot (time_filter: None)\n",
      "  Scraped 1000 posts from r/SuicideWatch\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1100 posts from r/SuicideWatch\n",
      "  Scraped 1200 posts from r/SuicideWatch\n",
      "  Scraped 1300 posts from r/SuicideWatch\n",
      "  Scraped 1400 posts from r/SuicideWatch\n",
      "  Scraped 1500 posts from r/SuicideWatch\n",
      "  Scraped 1600 posts from r/SuicideWatch\n",
      "  Scraped 1700 posts from r/SuicideWatch\n",
      "  Scraped 1800 posts from r/SuicideWatch\n",
      "  Scraped 1900 posts from r/SuicideWatch\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/SuicideWatch\n",
      "  Scraped 2100 posts from r/SuicideWatch\n",
      "  Scraped 2200 posts from r/SuicideWatch\n",
      "  Scraped 2300 posts from r/SuicideWatch\n",
      "  Scraped 2400 posts from r/SuicideWatch\n",
      "  Scraped 2500 posts from r/SuicideWatch\n",
      "  Scraped 2600 posts from r/SuicideWatch\n",
      "  Scraped 2700 posts from r/SuicideWatch\n",
      "  Scraped 2800 posts from r/SuicideWatch\n",
      "  Scraped 2900 posts from r/SuicideWatch\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/SuicideWatch\n",
      "  Scraped 3100 posts from r/SuicideWatch\n",
      "  Scraped 3200 posts from r/SuicideWatch\n",
      "  Scraped 3300 posts from r/SuicideWatch\n",
      "  Scraped 3400 posts from r/SuicideWatch\n",
      "  Scraped 3500 posts from r/SuicideWatch\n",
      "  Scraped 3600 posts from r/SuicideWatch\n",
      "  Scraped 3700 posts from r/SuicideWatch\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/SuicideWatch. Total posts: 3794\n",
      "Scraping r/OCD...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/OCD\n",
      "  Scraped 200 posts from r/OCD\n",
      "  Scraped 300 posts from r/OCD\n",
      "  Scraped 400 posts from r/OCD\n",
      "  Scraped 500 posts from r/OCD\n",
      "  Scraped 600 posts from r/OCD\n",
      "  Scraped 700 posts from r/OCD\n",
      "  Scraped 800 posts from r/OCD\n",
      "  Scraped 900 posts from r/OCD\n",
      "  Using method: hot (time_filter: None)\n",
      "  Scraped 1000 posts from r/OCD\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1100 posts from r/OCD\n",
      "  Scraped 1200 posts from r/OCD\n",
      "  Scraped 1300 posts from r/OCD\n",
      "  Scraped 1400 posts from r/OCD\n",
      "  Scraped 1500 posts from r/OCD\n",
      "  Scraped 1600 posts from r/OCD\n",
      "  Scraped 1700 posts from r/OCD\n",
      "  Scraped 1800 posts from r/OCD\n",
      "  Scraped 1900 posts from r/OCD\n",
      "  Scraped 2000 posts from r/OCD\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2100 posts from r/OCD\n",
      "  Scraped 2200 posts from r/OCD\n",
      "  Scraped 2300 posts from r/OCD\n",
      "  Scraped 2400 posts from r/OCD\n",
      "  Scraped 2500 posts from r/OCD\n",
      "  Scraped 2600 posts from r/OCD\n",
      "  Scraped 2700 posts from r/OCD\n",
      "  Scraped 2800 posts from r/OCD\n",
      "  Scraped 2900 posts from r/OCD\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/OCD\n",
      "  Scraped 3100 posts from r/OCD\n",
      "  Scraped 3200 posts from r/OCD\n",
      "  Scraped 3300 posts from r/OCD\n",
      "  Scraped 3400 posts from r/OCD\n",
      "  Scraped 3500 posts from r/OCD\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/OCD. Total posts: 3577\n",
      "Scraping r/AnxietyDepression...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/AnxietyDepression\n",
      "  Scraped 200 posts from r/AnxietyDepression\n",
      "  Scraped 300 posts from r/AnxietyDepression\n",
      "  Scraped 400 posts from r/AnxietyDepression\n",
      "  Scraped 500 posts from r/AnxietyDepression\n",
      "  Scraped 600 posts from r/AnxietyDepression\n",
      "  Scraped 700 posts from r/AnxietyDepression\n",
      "  Scraped 800 posts from r/AnxietyDepression\n",
      "  Scraped 900 posts from r/AnxietyDepression\n",
      "  Using method: hot (time_filter: None)\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1000 posts from r/AnxietyDepression\n",
      "  Scraped 1100 posts from r/AnxietyDepression\n",
      "  Scraped 1200 posts from r/AnxietyDepression\n",
      "  Scraped 1300 posts from r/AnxietyDepression\n",
      "  Scraped 1400 posts from r/AnxietyDepression\n",
      "  Scraped 1500 posts from r/AnxietyDepression\n",
      "  Scraped 1600 posts from r/AnxietyDepression\n",
      "  Scraped 1700 posts from r/AnxietyDepression\n",
      "  Scraped 1800 posts from r/AnxietyDepression\n",
      "  Scraped 1900 posts from r/AnxietyDepression\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/AnxietyDepression\n",
      "  Scraped 2100 posts from r/AnxietyDepression\n",
      "  Scraped 2200 posts from r/AnxietyDepression\n",
      "  Scraped 2300 posts from r/AnxietyDepression\n",
      "  Scraped 2400 posts from r/AnxietyDepression\n",
      "  Scraped 2500 posts from r/AnxietyDepression\n",
      "  Scraped 2600 posts from r/AnxietyDepression\n",
      "  Scraped 2700 posts from r/AnxietyDepression\n",
      "  Using method: top (time_filter: month)\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/AnxietyDepression. Total posts: 2764\n",
      "Scraping r/ptsd...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/ptsd\n",
      "  Scraped 200 posts from r/ptsd\n",
      "  Scraped 300 posts from r/ptsd\n",
      "  Scraped 400 posts from r/ptsd\n",
      "  Scraped 500 posts from r/ptsd\n",
      "  Scraped 600 posts from r/ptsd\n",
      "  Scraped 700 posts from r/ptsd\n",
      "  Scraped 800 posts from r/ptsd\n",
      "  Scraped 900 posts from r/ptsd\n",
      "  Using method: hot (time_filter: None)\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1000 posts from r/ptsd\n",
      "  Scraped 1100 posts from r/ptsd\n",
      "  Scraped 1200 posts from r/ptsd\n",
      "  Scraped 1300 posts from r/ptsd\n",
      "  Scraped 1400 posts from r/ptsd\n",
      "  Scraped 1500 posts from r/ptsd\n",
      "  Scraped 1600 posts from r/ptsd\n",
      "  Scraped 1700 posts from r/ptsd\n",
      "  Scraped 1800 posts from r/ptsd\n",
      "  Scraped 1900 posts from r/ptsd\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/ptsd\n",
      "  Scraped 2100 posts from r/ptsd\n",
      "  Scraped 2200 posts from r/ptsd\n",
      "  Scraped 2300 posts from r/ptsd\n",
      "  Scraped 2400 posts from r/ptsd\n",
      "  Scraped 2500 posts from r/ptsd\n",
      "  Scraped 2600 posts from r/ptsd\n",
      "  Scraped 2700 posts from r/ptsd\n",
      "  Scraped 2800 posts from r/ptsd\n",
      "  Scraped 2900 posts from r/ptsd\n",
      "  Using method: top (time_filter: month)\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/ptsd. Total posts: 2940\n",
      "Scraping r/bipolar...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/bipolar\n",
      "  Scraped 200 posts from r/bipolar\n",
      "  Scraped 300 posts from r/bipolar\n",
      "  Scraped 400 posts from r/bipolar\n",
      "  Scraped 500 posts from r/bipolar\n",
      "  Scraped 600 posts from r/bipolar\n",
      "  Scraped 700 posts from r/bipolar\n",
      "  Scraped 800 posts from r/bipolar\n",
      "  Scraped 900 posts from r/bipolar\n",
      "  Using method: hot (time_filter: None)\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1000 posts from r/bipolar\n",
      "  Scraped 1100 posts from r/bipolar\n",
      "  Scraped 1200 posts from r/bipolar\n",
      "  Scraped 1300 posts from r/bipolar\n",
      "  Scraped 1400 posts from r/bipolar\n",
      "  Scraped 1500 posts from r/bipolar\n",
      "  Scraped 1600 posts from r/bipolar\n",
      "  Scraped 1700 posts from r/bipolar\n",
      "  Scraped 1800 posts from r/bipolar\n",
      "  Scraped 1900 posts from r/bipolar\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/bipolar\n",
      "  Scraped 2100 posts from r/bipolar\n",
      "  Scraped 2200 posts from r/bipolar\n",
      "  Scraped 2300 posts from r/bipolar\n",
      "  Scraped 2400 posts from r/bipolar\n",
      "  Scraped 2500 posts from r/bipolar\n",
      "  Scraped 2600 posts from r/bipolar\n",
      "  Scraped 2700 posts from r/bipolar\n",
      "  Scraped 2800 posts from r/bipolar\n",
      "  Scraped 2900 posts from r/bipolar\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/bipolar\n",
      "  Scraped 3100 posts from r/bipolar\n",
      "  Scraped 3200 posts from r/bipolar\n",
      "  Scraped 3300 posts from r/bipolar\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/bipolar. Total posts: 3378\n",
      "Scraping r/socialanxiety...\n",
      "  Using method: new (time_filter: None)\n",
      "  Scraped 100 posts from r/socialanxiety\n",
      "  Scraped 200 posts from r/socialanxiety\n",
      "  Scraped 300 posts from r/socialanxiety\n",
      "  Scraped 400 posts from r/socialanxiety\n",
      "  Scraped 500 posts from r/socialanxiety\n",
      "  Scraped 600 posts from r/socialanxiety\n",
      "  Scraped 700 posts from r/socialanxiety\n",
      "  Scraped 800 posts from r/socialanxiety\n",
      "  Scraped 900 posts from r/socialanxiety\n",
      "  Using method: hot (time_filter: None)\n",
      "  Using method: top (time_filter: all)\n",
      "  Scraped 1000 posts from r/socialanxiety\n",
      "  Scraped 1100 posts from r/socialanxiety\n",
      "  Scraped 1200 posts from r/socialanxiety\n",
      "  Scraped 1300 posts from r/socialanxiety\n",
      "  Scraped 1400 posts from r/socialanxiety\n",
      "  Scraped 1500 posts from r/socialanxiety\n",
      "  Scraped 1600 posts from r/socialanxiety\n",
      "  Scraped 1700 posts from r/socialanxiety\n",
      "  Scraped 1800 posts from r/socialanxiety\n",
      "  Scraped 1900 posts from r/socialanxiety\n",
      "  Using method: top (time_filter: year)\n",
      "  Scraped 2000 posts from r/socialanxiety\n",
      "  Scraped 2100 posts from r/socialanxiety\n",
      "  Scraped 2200 posts from r/socialanxiety\n",
      "  Scraped 2300 posts from r/socialanxiety\n",
      "  Scraped 2400 posts from r/socialanxiety\n",
      "  Scraped 2500 posts from r/socialanxiety\n",
      "  Scraped 2600 posts from r/socialanxiety\n",
      "  Scraped 2700 posts from r/socialanxiety\n",
      "  Scraped 2800 posts from r/socialanxiety\n",
      "  Scraped 2900 posts from r/socialanxiety\n",
      "  Using method: top (time_filter: month)\n",
      "  Scraped 3000 posts from r/socialanxiety\n",
      "  Scraped 3100 posts from r/socialanxiety\n",
      "  Using method: rising (time_filter: None)\n",
      "Finished scraping r/socialanxiety. Total posts: 3183\n",
      "Data saved to C:\\Users\\Asus\\OneDrive\\Desktop\\Sem2\\Research Practicum\\New\\mental_health_data.csv. Total posts: 34550\n",
      "   post_id   subreddit                                              title  \\\n",
      "0  1jlgpdc  depression  Can someone please help this guy ( AmoebaRepul...   \n",
      "1  1jlgoal  depression                                    Dopamine Crash?   \n",
      "2  1jlgo49  depression                How to help friend to do something?   \n",
      "3  1jlgi0m  depression                 thank you to anyone who reads this   \n",
      "4  1jlg9kb  depression                  Fun fact: I can't take it anymore   \n",
      "\n",
      "                                                text                author  \\\n",
      "0  Can someone please help this guy ( AmoebaRepul...           lucaloscuda   \n",
      "1  I have been living with depression for most of...       No_Product_6837   \n",
      "2   How did you overcome depression? When you’re ...  Objective_Branch_655   \n",
      "3  I feel like I’m going to lose my mind. I wish ...                nyuuhb   \n",
      "4  I'm useless, I've tried to stay strong but Low...  Shot_Entertainer3068   \n",
      "\n",
      "             timestamp  score  comment_count  \\\n",
      "0  2025-03-27T23:10:34      1              0   \n",
      "1  2025-03-27T23:09:19      1              0   \n",
      "2  2025-03-27T23:09:07      1              0   \n",
      "3  2025-03-27T23:01:55      2              0   \n",
      "4  2025-03-27T22:51:53      2              1   \n",
      "\n",
      "                                            comments location label  \n",
      "0                                                        None  None  \n",
      "1                                                        None  None  \n",
      "2                                                        None  None  \n",
      "3                                                        None  None  \n",
      "4  waking up in the morning really do be feeling ...     None  None  \n",
      "Next step: Manually label a subset of the 'label' column with 'positive', 'negative', or 'distress' for training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize Reddit instance with your credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"IffN842DjoBPAubXeob9ew\",           \n",
    "    client_secret=\"SZfgpnRXyE_wRi-wHuoGGdWNqHpe_g\",  \n",
    "    user_agent=\"python:MentalHealthScraper:v1.0 (by u/u/FamiliarAd3777)\"  \n",
    ")\n",
    "\n",
    "# Define subreddits and target\n",
    "subreddits = [\n",
    "    \"depression\", \"anxiety\", \"mentalhealth\", \"BPD\", \"SuicideWatch\",\n",
    "    \"OCD\", \"AnxietyDepression\", \"ptsd\", \"bipolar\", \"socialanxiety\"\n",
    "]\n",
    "target_posts = 50000\n",
    "posts_per_subreddit = target_posts // len(subreddits)\n",
    "\n",
    "# Data storage with new columns\n",
    "data = {\n",
    "    \"post_id\": [], \"subreddit\": [], \"title\": [], \"text\": [], \"author\": [],\n",
    "    \"timestamp\": [], \"score\": [], \"comment_count\": [], \"comments\": [],\n",
    "    \"location\": [],  # For geospatial analysis\n",
    "    \"label\": []      # For training/evaluation (to be filled later)\n",
    "}\n",
    "\n",
    "# List of specific locations to search for\n",
    "specific_locations = [\n",
    "    \"Belfast\", \"Derry\", \"Dundalk\", \"Limerick\", \"Dublin\", \"Cork\", \"Galway\",\n",
    "    \"Waterford\", \"Newtownabbey\", \"Bangor\", \"London\", \"Manchester\", \n",
    "    \"Birmingham\", \"Leeds\", \"Glasgow\"\n",
    "]\n",
    "\n",
    "# Function to extract specific locations from text\n",
    "def extract_location(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return None\n",
    "    text_lower = text.lower()  # Case-insensitive matching\n",
    "    for location in specific_locations:\n",
    "        if location.lower() in text_lower:\n",
    "            return location\n",
    "    return None\n",
    "\n",
    "# Set to keep track of unique post IDs (to avoid duplicates)\n",
    "seen_post_ids = set()\n",
    "\n",
    "# Scrape data using multiple methods\n",
    "for subreddit_name in subreddits:\n",
    "    print(f\"Scraping r/{subreddit_name}...\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    post_count = 0\n",
    "\n",
    "    # Define methods to scrape from\n",
    "    methods = [\n",
    "        (\"new\", None),\n",
    "        (\"hot\", None),\n",
    "        (\"top\", \"all\"),\n",
    "        (\"top\", \"year\"),\n",
    "        (\"top\", \"month\"),\n",
    "        (\"rising\", None)\n",
    "    ]\n",
    "\n",
    "    for method, time_filter in methods:\n",
    "        if post_count >= posts_per_subreddit:\n",
    "            break\n",
    "        print(f\"  Using method: {method} (time_filter: {time_filter})\")\n",
    "        \n",
    "        # Select the method dynamically\n",
    "        if method == \"new\":\n",
    "            submissions = subreddit.new(limit=None)\n",
    "        elif method == \"hot\":\n",
    "            submissions = subreddit.hot(limit=None)\n",
    "        elif method == \"top\":\n",
    "            submissions = subreddit.top(time_filter=time_filter, limit=None)\n",
    "        elif method == \"rising\":\n",
    "            submissions = subreddit.rising(limit=None)\n",
    "\n",
    "        for submission in submissions:\n",
    "            # Skip if post ID already seen (avoid duplicates)\n",
    "            if submission.id in seen_post_ids:\n",
    "                continue\n",
    "            seen_post_ids.add(submission.id)\n",
    "\n",
    "            try:\n",
    "                data[\"post_id\"].append(submission.id)\n",
    "                data[\"subreddit\"].append(subreddit_name)\n",
    "                data[\"title\"].append(submission.title)\n",
    "                data[\"text\"].append(submission.selftext)\n",
    "                data[\"author\"].append(str(submission.author) if submission.author else \"[deleted]\")\n",
    "                data[\"timestamp\"].append(datetime.fromtimestamp(submission.created_utc).isoformat())\n",
    "                data[\"score\"].append(submission.score)\n",
    "                submission.comments.replace_more(limit=0)\n",
    "                comments = [comment.body for comment in submission.comments.list()[:10]]\n",
    "                data[\"comment_count\"].append(len(comments))\n",
    "                data[\"comments\"].append(\" | \".join(comments))\n",
    "                \n",
    "                # Extract location from title, text, or comments\n",
    "                combined_text = f\"{submission.title} {submission.selftext} {' '.join(comments)}\"\n",
    "                location = extract_location(combined_text)\n",
    "                data[\"location\"].append(location)\n",
    "                \n",
    "                # Placeholder for label (to be filled manually later)\n",
    "                data[\"label\"].append(None)  # Will be filled after scraping\n",
    "                \n",
    "                post_count += 1\n",
    "                if post_count % 100 == 0:\n",
    "                    print(f\"  Scraped {post_count} posts from r/{subreddit_name}\")\n",
    "                if post_count >= posts_per_subreddit:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping post {submission.id}: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    print(f\"Finished scraping r/{subreddit_name}. Total posts: {post_count}\")\n",
    "\n",
    "# Specify the save location\n",
    "save_directory = \"C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\Sem2\\\\Research Practicum\\\\New\"\n",
    "output_filename = \"mental_health_data.csv\"\n",
    "output_path = os.path.join(save_directory, output_filename)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "    print(f\"Created directory: {save_directory}\")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}. Total posts: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "# Reminder to manually label the data\n",
    "print(\"Next step: Manually label a subset of the 'label' column with 'positive', 'negative', or 'distress' for training and evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import os\n",
    "# Load your saved data\n",
    "file_path = \"C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\Sem2\\\\Research Practicum\\\\New\\\\mental_health_data.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentiment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentiment analysis pipeline with PyTorch framework\n",
    "print(\"Loading sentiment model...\")\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    framework=\"pt\"  # Explicitly use PyTorch to avoid TensorFlow/Keras issues\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distress keywords\n",
    "distress_keywords = [\"help\", \"crisis\", \"suicide\", \"panic\", \"desperate\", \"kill\", \"hopeless\", \"emergency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify text and integrate score\n",
    "def classify_text(text, score):\n",
    "    # Handle empty or NaN text\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Truncate text to 512 tokens (model limit)\n",
    "    combined_text = text[:512]\n",
    "    \n",
    "    # Get sentiment prediction\n",
    "    result = sentiment_classifier(combined_text)[0]\n",
    "    label = result[\"label\"].lower()  # 'positive' or 'negative'\n",
    "    confidence = result[\"score\"]\n",
    "    \n",
    "    # Convert text to lowercase for keyword matching\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Custom logic with distress keywords and score\n",
    "    if any(keyword in text_lower for keyword in distress_keywords):\n",
    "        if score > 1000:  # High score indicates significant distress\n",
    "            return \"distress\"\n",
    "        return \"negative\"  # Default to negative if distress keywords present but score is low\n",
    "    \n",
    "    # Refine sentiment with confidence and score\n",
    "    if confidence > 0.7:  # Only accept high-confidence predictions\n",
    "        if label == \"negative\" and score > 5000:  # High score + negative = potential distress\n",
    "            return \"distress\" if \"help\" in text_lower else \"negative\"\n",
    "        elif label == \"positive\" and score > 500:  # High score reinforces positive\n",
    "            return \"positive\"\n",
    "        return label\n",
    "    return None  # Return None if confidence is too low\n",
    "\n",
    "# Batch processing to avoid memory issues\n",
    "def batch_classify(df, batch_size=100):\n",
    "    print(f\"Processing {len(df)} posts in batches of {batch_size}...\")\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i + batch_size]\n",
    "        df.loc[batch.index, 'label'] = batch.apply(\n",
    "            lambda row: classify_text(f\"{row['title']} {row['text']} {row['comments']}\", row['score']), axis=1\n",
    "        )\n",
    "        print(f\"Processed {min(i + batch_size, len(df))} posts...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 34550 posts in batches of 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_18012\\3040245353.py:38: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' None 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' None 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[batch.index, 'label'] = batch.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 posts...\n",
      "Processed 200 posts...\n",
      "Processed 300 posts...\n",
      "Processed 400 posts...\n",
      "Processed 500 posts...\n",
      "Processed 600 posts...\n",
      "Processed 700 posts...\n",
      "Processed 800 posts...\n",
      "Processed 900 posts...\n",
      "Processed 1000 posts...\n",
      "Processed 1100 posts...\n",
      "Processed 1200 posts...\n",
      "Processed 1300 posts...\n",
      "Processed 1400 posts...\n",
      "Processed 1500 posts...\n",
      "Processed 1600 posts...\n",
      "Processed 1700 posts...\n",
      "Processed 1800 posts...\n",
      "Processed 1900 posts...\n",
      "Processed 2000 posts...\n",
      "Processed 2100 posts...\n",
      "Processed 2200 posts...\n",
      "Processed 2300 posts...\n",
      "Processed 2400 posts...\n",
      "Processed 2500 posts...\n",
      "Processed 2600 posts...\n",
      "Processed 2700 posts...\n",
      "Processed 2800 posts...\n",
      "Processed 2900 posts...\n",
      "Processed 3000 posts...\n",
      "Processed 3100 posts...\n",
      "Processed 3200 posts...\n",
      "Processed 3300 posts...\n",
      "Processed 3400 posts...\n",
      "Processed 3500 posts...\n",
      "Processed 3600 posts...\n",
      "Processed 3700 posts...\n",
      "Processed 3800 posts...\n",
      "Processed 3900 posts...\n",
      "Processed 4000 posts...\n",
      "Processed 4100 posts...\n",
      "Processed 4200 posts...\n",
      "Processed 4300 posts...\n",
      "Processed 4400 posts...\n",
      "Processed 4500 posts...\n",
      "Processed 4600 posts...\n",
      "Processed 4700 posts...\n",
      "Processed 4800 posts...\n",
      "Processed 4900 posts...\n",
      "Processed 5000 posts...\n",
      "Processed 5100 posts...\n",
      "Processed 5200 posts...\n",
      "Processed 5300 posts...\n",
      "Processed 5400 posts...\n",
      "Processed 5500 posts...\n",
      "Processed 5600 posts...\n",
      "Processed 5700 posts...\n",
      "Processed 5800 posts...\n",
      "Processed 5900 posts...\n",
      "Processed 6000 posts...\n",
      "Processed 6100 posts...\n",
      "Processed 6200 posts...\n",
      "Processed 6300 posts...\n",
      "Processed 6400 posts...\n",
      "Processed 6500 posts...\n",
      "Processed 6600 posts...\n",
      "Processed 6700 posts...\n",
      "Processed 6800 posts...\n",
      "Processed 6900 posts...\n",
      "Processed 7000 posts...\n",
      "Processed 7100 posts...\n",
      "Processed 7200 posts...\n",
      "Processed 7300 posts...\n",
      "Processed 7400 posts...\n",
      "Processed 7500 posts...\n",
      "Processed 7600 posts...\n",
      "Processed 7700 posts...\n",
      "Processed 7800 posts...\n",
      "Processed 7900 posts...\n",
      "Processed 8000 posts...\n",
      "Processed 8100 posts...\n",
      "Processed 8200 posts...\n",
      "Processed 8300 posts...\n",
      "Processed 8400 posts...\n",
      "Processed 8500 posts...\n",
      "Processed 8600 posts...\n",
      "Processed 8700 posts...\n",
      "Processed 8800 posts...\n",
      "Processed 8900 posts...\n",
      "Processed 9000 posts...\n",
      "Processed 9100 posts...\n",
      "Processed 9200 posts...\n",
      "Processed 9300 posts...\n",
      "Processed 9400 posts...\n",
      "Processed 9500 posts...\n",
      "Processed 9600 posts...\n",
      "Processed 9700 posts...\n",
      "Processed 9800 posts...\n",
      "Processed 9900 posts...\n",
      "Processed 10000 posts...\n",
      "Processed 10100 posts...\n",
      "Processed 10200 posts...\n",
      "Processed 10300 posts...\n",
      "Processed 10400 posts...\n",
      "Processed 10500 posts...\n",
      "Processed 10600 posts...\n",
      "Processed 10700 posts...\n",
      "Processed 10800 posts...\n",
      "Processed 10900 posts...\n",
      "Processed 11000 posts...\n",
      "Processed 11100 posts...\n",
      "Processed 11200 posts...\n",
      "Processed 11300 posts...\n",
      "Processed 11400 posts...\n",
      "Processed 11500 posts...\n",
      "Processed 11600 posts...\n",
      "Processed 11700 posts...\n",
      "Processed 11800 posts...\n",
      "Processed 11900 posts...\n",
      "Processed 12000 posts...\n",
      "Processed 12100 posts...\n",
      "Processed 12200 posts...\n",
      "Processed 12300 posts...\n",
      "Processed 12400 posts...\n",
      "Processed 12500 posts...\n",
      "Processed 12600 posts...\n",
      "Processed 12700 posts...\n",
      "Processed 12800 posts...\n",
      "Processed 12900 posts...\n",
      "Processed 13000 posts...\n",
      "Processed 13100 posts...\n",
      "Processed 13200 posts...\n",
      "Processed 13300 posts...\n",
      "Processed 13400 posts...\n",
      "Processed 13500 posts...\n",
      "Processed 13600 posts...\n",
      "Processed 13700 posts...\n",
      "Processed 13800 posts...\n",
      "Processed 13900 posts...\n",
      "Processed 14000 posts...\n",
      "Processed 14100 posts...\n",
      "Processed 14200 posts...\n",
      "Processed 14300 posts...\n",
      "Processed 14400 posts...\n",
      "Processed 14500 posts...\n",
      "Processed 14600 posts...\n",
      "Processed 14700 posts...\n",
      "Processed 14800 posts...\n",
      "Processed 14900 posts...\n",
      "Processed 15000 posts...\n",
      "Processed 15100 posts...\n",
      "Processed 15200 posts...\n",
      "Processed 15300 posts...\n",
      "Processed 15400 posts...\n",
      "Processed 15500 posts...\n",
      "Processed 15600 posts...\n",
      "Processed 15700 posts...\n",
      "Processed 15800 posts...\n",
      "Processed 15900 posts...\n",
      "Processed 16000 posts...\n",
      "Processed 16100 posts...\n",
      "Processed 16200 posts...\n",
      "Processed 16300 posts...\n",
      "Processed 16400 posts...\n",
      "Processed 16500 posts...\n",
      "Processed 16600 posts...\n",
      "Processed 16700 posts...\n",
      "Processed 16800 posts...\n",
      "Processed 16900 posts...\n",
      "Processed 17000 posts...\n",
      "Processed 17100 posts...\n",
      "Processed 17200 posts...\n",
      "Processed 17300 posts...\n",
      "Processed 17400 posts...\n",
      "Processed 17500 posts...\n",
      "Processed 17600 posts...\n",
      "Processed 17700 posts...\n",
      "Processed 17800 posts...\n",
      "Processed 17900 posts...\n",
      "Processed 18000 posts...\n",
      "Processed 18100 posts...\n",
      "Processed 18200 posts...\n",
      "Processed 18300 posts...\n",
      "Processed 18400 posts...\n",
      "Processed 18500 posts...\n",
      "Processed 18600 posts...\n",
      "Processed 18700 posts...\n",
      "Processed 18800 posts...\n",
      "Processed 18900 posts...\n",
      "Processed 19000 posts...\n",
      "Processed 19100 posts...\n",
      "Processed 19200 posts...\n",
      "Processed 19300 posts...\n",
      "Processed 19400 posts...\n",
      "Processed 19500 posts...\n",
      "Processed 19600 posts...\n",
      "Processed 19700 posts...\n",
      "Processed 19800 posts...\n",
      "Processed 19900 posts...\n",
      "Processed 20000 posts...\n",
      "Processed 20100 posts...\n",
      "Processed 20200 posts...\n",
      "Processed 20300 posts...\n",
      "Processed 20400 posts...\n",
      "Processed 20500 posts...\n",
      "Processed 20600 posts...\n",
      "Processed 20700 posts...\n",
      "Processed 20800 posts...\n",
      "Processed 20900 posts...\n",
      "Processed 21000 posts...\n",
      "Processed 21100 posts...\n",
      "Processed 21200 posts...\n",
      "Processed 21300 posts...\n",
      "Processed 21400 posts...\n",
      "Processed 21500 posts...\n",
      "Processed 21600 posts...\n",
      "Processed 21700 posts...\n",
      "Processed 21800 posts...\n",
      "Processed 21900 posts...\n",
      "Processed 22000 posts...\n",
      "Processed 22100 posts...\n",
      "Processed 22200 posts...\n",
      "Processed 22300 posts...\n",
      "Processed 22400 posts...\n",
      "Processed 22500 posts...\n",
      "Processed 22600 posts...\n",
      "Processed 22700 posts...\n",
      "Processed 22800 posts...\n",
      "Processed 22900 posts...\n",
      "Processed 23000 posts...\n",
      "Processed 23100 posts...\n",
      "Processed 23200 posts...\n",
      "Processed 23300 posts...\n",
      "Processed 23400 posts...\n",
      "Processed 23500 posts...\n",
      "Processed 23600 posts...\n",
      "Processed 23700 posts...\n",
      "Processed 23800 posts...\n",
      "Processed 23900 posts...\n",
      "Processed 24000 posts...\n",
      "Processed 24100 posts...\n",
      "Processed 24200 posts...\n",
      "Processed 24300 posts...\n",
      "Processed 24400 posts...\n",
      "Processed 24500 posts...\n",
      "Processed 24600 posts...\n",
      "Processed 24700 posts...\n",
      "Processed 24800 posts...\n",
      "Processed 24900 posts...\n",
      "Processed 25000 posts...\n",
      "Processed 25100 posts...\n",
      "Processed 25200 posts...\n",
      "Processed 25300 posts...\n",
      "Processed 25400 posts...\n",
      "Processed 25500 posts...\n",
      "Processed 25600 posts...\n",
      "Processed 25700 posts...\n",
      "Processed 25800 posts...\n",
      "Processed 25900 posts...\n",
      "Processed 26000 posts...\n",
      "Processed 26100 posts...\n",
      "Processed 26200 posts...\n",
      "Processed 26300 posts...\n",
      "Processed 26400 posts...\n",
      "Processed 26500 posts...\n",
      "Processed 26600 posts...\n",
      "Processed 26700 posts...\n",
      "Processed 26800 posts...\n",
      "Processed 26900 posts...\n",
      "Processed 27000 posts...\n",
      "Processed 27100 posts...\n",
      "Processed 27200 posts...\n",
      "Processed 27300 posts...\n",
      "Processed 27400 posts...\n",
      "Processed 27500 posts...\n",
      "Processed 27600 posts...\n",
      "Processed 27700 posts...\n",
      "Processed 27800 posts...\n",
      "Processed 27900 posts...\n",
      "Processed 28000 posts...\n",
      "Processed 28100 posts...\n",
      "Processed 28200 posts...\n",
      "Processed 28300 posts...\n",
      "Processed 28400 posts...\n",
      "Processed 28500 posts...\n",
      "Processed 28600 posts...\n",
      "Processed 28700 posts...\n",
      "Processed 28800 posts...\n",
      "Processed 28900 posts...\n",
      "Processed 29000 posts...\n",
      "Processed 29100 posts...\n",
      "Processed 29200 posts...\n",
      "Processed 29300 posts...\n",
      "Processed 29400 posts...\n",
      "Processed 29500 posts...\n",
      "Processed 29600 posts...\n",
      "Processed 29700 posts...\n",
      "Processed 29800 posts...\n",
      "Processed 29900 posts...\n",
      "Processed 30000 posts...\n",
      "Processed 30100 posts...\n",
      "Processed 30200 posts...\n",
      "Processed 30300 posts...\n",
      "Processed 30400 posts...\n",
      "Processed 30500 posts...\n",
      "Processed 30600 posts...\n",
      "Processed 30700 posts...\n",
      "Processed 30800 posts...\n",
      "Processed 30900 posts...\n",
      "Processed 31000 posts...\n",
      "Processed 31100 posts...\n",
      "Processed 31200 posts...\n",
      "Processed 31300 posts...\n",
      "Processed 31400 posts...\n",
      "Processed 31500 posts...\n",
      "Processed 31600 posts...\n",
      "Processed 31700 posts...\n",
      "Processed 31800 posts...\n",
      "Processed 31900 posts...\n",
      "Processed 32000 posts...\n",
      "Processed 32100 posts...\n",
      "Processed 32200 posts...\n",
      "Processed 32300 posts...\n",
      "Processed 32400 posts...\n",
      "Processed 32500 posts...\n",
      "Processed 32600 posts...\n",
      "Processed 32700 posts...\n",
      "Processed 32800 posts...\n",
      "Processed 32900 posts...\n",
      "Processed 33000 posts...\n",
      "Processed 33100 posts...\n",
      "Processed 33200 posts...\n",
      "Processed 33300 posts...\n",
      "Processed 33400 posts...\n",
      "Processed 33500 posts...\n",
      "Processed 33600 posts...\n",
      "Processed 33700 posts...\n",
      "Processed 33800 posts...\n",
      "Processed 33900 posts...\n",
      "Processed 34000 posts...\n",
      "Processed 34100 posts...\n",
      "Processed 34200 posts...\n",
      "Processed 34300 posts...\n",
      "Processed 34400 posts...\n",
      "Processed 34500 posts...\n",
      "Processed 34550 posts...\n"
     ]
    }
   ],
   "source": [
    "# Apply classification\n",
    "batch_classify(df, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data saved to C:\\Users\\Asus\\OneDrive\\Desktop\\Sem2\\Research Practicum\\New\\mental_health_data_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the updated DataFrame\n",
    "output_path = \"C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\Sem2\\\\Research Practicum\\\\New\\\\mental_health_data_labeled.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Labeled data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution:\n",
      "label\n",
      "negative    29842\n",
      "distress     2202\n",
      "positive     2130\n",
      "None          376\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of labeled data:\n",
      "                                               title  score     label\n",
      "0  Can someone please help this guy ( AmoebaRepul...      1  negative\n",
      "1                                    Dopamine Crash?      1  negative\n",
      "2                How to help friend to do something?      1  negative\n",
      "3                 thank you to anyone who reads this      2  negative\n",
      "4                  Fun fact: I can't take it anymore      2  negative\n",
      "5            Today’s not bad. Is it just false hope?      1  negative\n",
      "6  Everyone keeps telling me how well I’m doing b...      1  negative\n",
      "7                                        im so tired      1  negative\n",
      "8                           Literally just gonna kms      1  negative\n",
      "9  killing myself. wilI be better than staying Io...      2  negative\n"
     ]
    }
   ],
   "source": [
    "# Optional: Display a sample of labeled data\n",
    "print(\"\\nSample of labeled data:\")\n",
    "print(df[['title', 'score', 'label']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data Cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the labeled dataset\n",
    "file_path = \"C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\Sem2\\\\Research Practicum\\\\New\\\\mental_health_data_labeled.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial missing values:\n",
      " post_id              0\n",
      "subreddit            0\n",
      "title                0\n",
      "text              3612\n",
      "author               0\n",
      "timestamp            0\n",
      "score                0\n",
      "comment_count        0\n",
      "comments          3014\n",
      "location         34456\n",
      "label              376\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Handle Missing Values\n",
    "print(\"Initial missing values:\\n\", df.isnull().sum())\n",
    "df['title'] = df['title'].fillna(\"\")  # Empty string for missing titles\n",
    "df['text'] = df['text'].fillna(\"\")    # Empty string for missing text\n",
    "df['comments'] = df['comments'].fillna(\"\")  # Empty string for missing comments\n",
    "df['label'] = df['label'].fillna(\"None\")  # Flag unlabeled rows\n",
    "df['location'] = df['location'].fillna(\"British Isles\")  # Flag missing locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after removing duplicates: 34550\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Remove Duplicates\n",
    "df = df.drop_duplicates(subset=['post_id'], keep='first')\n",
    "print(f\"Rows after removing duplicates: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean Text Data\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['comments'] = df['comments'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Standardize Timestamps\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score stats before cleaning:\n",
      " count    34550.000000\n",
      "mean       329.019595\n",
      "std        746.044201\n",
      "min          0.000000\n",
      "25%          4.000000\n",
      "50%         43.000000\n",
      "75%        284.000000\n",
      "max      27945.000000\n",
      "Name: score, dtype: float64\n",
      "Score stats after cleaning:\n",
      " count    34550.000000\n",
      "mean       329.019595\n",
      "std        746.044201\n",
      "min          0.000000\n",
      "25%          4.000000\n",
      "50%         43.000000\n",
      "75%        284.000000\n",
      "max      27945.000000\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Handle Outliers in Score\n",
    "print(\"Score stats before cleaning:\\n\", df['score'].describe())\n",
    "df = df[df['score'].between(0, 28000)]  # Remove negative or extreme outliers\n",
    "print(\"Score stats after cleaning:\\n\", df['score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Normalize Labels\n",
    "df['label'] = df['label'].str.lower()  # Ensure consistent casing\n",
    "valid_labels = ['positive', 'negative', 'distress']\n",
    "df = df[df['label'].isin(valid_labels)]  # Drop invalid labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Filter Irrelevant Data\n",
    "df = df[df['author'] != '[deleted]']  # Remove deleted authors\n",
    "df = df[df['title'].str.len() > 5]   # Keep titles longer than 5 chars\n",
    "df = df[df['text'].str.len() > 10]   # Keep text longer than 10 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Simplify Comments (optional: split into list)\n",
    "df['comments'] = df['comments'].str.split(\" | \")  # Split comments into list\n",
    "df['comment_count_cleaned'] = df['comments'].apply(len)  # Update comment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Validate Location\n",
    "df['location'] = df['location'].str.lower().str.strip()\n",
    "valid_locations = [\n",
    "    \"belfast\", \"derry\", \"dundalk\", \"limerick\", \"dublin\", \"cork\", \"galway\",\n",
    "    \"waterford\", \"newtownabbey\", \"bangor\", \"london\", \"manchester\", \n",
    "    \"birmingham\", \"leeds\", \"glasgow\", \"british isles\"\n",
    "]\n",
    "df['location'] = df['location'].apply(lambda x: x if x in valid_locations else \"british isles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Final Data Validation\n",
    "# Handle NaN or non-numeric values in 'score' and 'comment_count' before conversion\n",
    "df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0).astype(int)  # Convert to int, NaN -> 0\n",
    "df['comment_count'] = pd.to_numeric(df['comment_count'], errors='coerce').fillna(0).astype(int)  # Convert to int, NaN -> 0\n",
    "df = df.dropna(subset=['timestamp'])  # Drop rows with invalid timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to C:\\Users\\Asus\\OneDrive\\Desktop\\Sem2\\Research Practicum\\New\\mental_health_data_cleaned.csv\n",
      "Final shape: (27808, 12)\n",
      "Final missing values:\n",
      " post_id                  0\n",
      "subreddit                0\n",
      "title                    0\n",
      "text                     0\n",
      "author                   0\n",
      "timestamp                0\n",
      "score                    0\n",
      "comment_count            0\n",
      "comments                 0\n",
      "location                 0\n",
      "label                    0\n",
      "comment_count_cleaned    0\n",
      "dtype: int64\n",
      "Label distribution:\n",
      " label\n",
      "negative    25323\n",
      "positive     1350\n",
      "distress     1135\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "output_path = \"C:\\\\Users\\\\Asus\\\\OneDrive\\\\Desktop\\\\Sem2\\\\Research Practicum\\\\New\\\\mental_health_data_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data saved to {output_path}\")\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "print(\"Final missing values:\\n\", df.isnull().sum())\n",
    "print(\"Label distribution:\\n\", df['label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
